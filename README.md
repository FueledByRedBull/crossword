# ğŸ§© Wikipedia-Seeded Thematic Crossword Generator

Generate coherent, educational crossword puzzles from any Wikipedia article. Give it a seed topic like *Thermodynamics* or *Jazz*, and it builds a complete puzzle â€” grid, clues, and all.

## How It Works

The system runs a multi-stage pipeline:

1. **Seed Graph Expansion** â€” Fetches outgoing links from the seed Wikipedia article (optional 2-hop expansion)
2. **Semantic Scoring** â€” Ranks candidates using TF-IDF cosine similarity, redundancy penalty, depth penalty, and backlink bonus (MMR-style)
3. **K Optimization** â€” Selects the optimal number of articles using a crossword-aware objective with diminishing-returns stopping
4. **Term Extraction** â€” Extracts crossword answer candidates via spaCy NLP (with nltk fallback), lead-section boldface signals, and quality filters
5. **Clue Generation** â€” Four-pass pipeline: extract â†’ mask/trim â†’ validate (leakage check) â†’ diversity deduplication
6. **Grid Topology Selection** â€” Scores candidate grid templates against the word-length distribution
7. **CSP Fill** â€” Fills the grid using arc consistency (AC-3), MRV/degree variable ordering, and forward checking with restarts
8. **Provenance & Packaging** â€” Bundles the puzzle with full CC BY-SA attribution metadata

## Quick Start

### Prerequisites

- Python 3.10+

### Installation

```bash
pip install -r requirements.txt
```

### Usage

```bash
# Generate an English crossword from "Thermodynamics"
python cli.py generate \
  --seed "Thermodynamics" \
  --lang en \
  --grid-size 15 \
  --output outputs/thermo_15.json

# Generate a Greek crossword
python cli.py generate \
  --seed "Î˜ÎµÏÎ¼Î¿Î´Ï…Î½Î±Î¼Î¹ÎºÎ®" \
  --lang el \
  --grid-size 13 \
  --output outputs/thermo_el_13.json
```

**Key flags:**
| Flag | Description | Default |
|------|-------------|---------|
| `--seed` | Wikipedia article title | *(required)* |
| `--lang` | Language (`en` or `el`) | `en` |
| `--grid-size` | Grid dimension | `15` |
| `--expansion` | `one_hop_only` or `one_hop_plus_bounded_two_hop` | `one_hop_only` |
| `--output` | Output path | `outputs/puzzle.json` |

## Project Structure

```
src/
â”œâ”€â”€ pipeline.py          # Stage orchestration
â”œâ”€â”€ wiki_client.py       # MediaWiki API client with caching
â”œâ”€â”€ wikidata_client.py   # Wikidata entity lookups
â”œâ”€â”€ semantic.py          # TF-IDF vectorization & MMR scoring
â”œâ”€â”€ k_selector.py        # Crossword-aware K optimization
â”œâ”€â”€ term_extractor.py    # NLP-based answer extraction
â”œâ”€â”€ clue_builder.py      # Four-pass clue pipeline
â”œâ”€â”€ topology.py          # Grid template generation & scoring
â”œâ”€â”€ crossword_csp.py     # Constraint solver (AC-3 + backtracking)
â”œâ”€â”€ provenance.py        # Attribution capture
â”œâ”€â”€ text_normalize.py    # Text cleaning & normalization
â”œâ”€â”€ cache.py             # Disk cache layer
â”œâ”€â”€ diagnostics.py       # Diagnostics emission
â””â”€â”€ __init__.py

tests/                   # Unit & integration tests
scripts/
â””â”€â”€ bench.py             # Benchmarking utility
cli.py                   # CLI entry point
PLAN.md                  # Detailed design document
requirements.txt         # Pinned dependencies
```

## Output

Each run produces:
- **`puzzle.json`** â€” Grid, clues, and fill status
- **`diagnostics.json`** â€” Scores, selection decisions, and solver trace
- **`candidate_scores.csv`** â€” Ranked article candidates
- **`k_selection_trace.csv`** â€” Marginal utility trace
- **`attribution.json`** â€” Per-clue Wikipedia revision provenance

## Dependencies

- [spaCy](https://spacy.io/) â€” NLP backbone for term extraction
- [mwparserfromhell](https://github.com/earwig/mwparserfromhell) â€” Wikipedia markup parsing
- MediaWiki API â€” Content and metadata (no scraping)

## License

Content derived from Wikipedia is used under [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/). Attribution metadata is bundled with every generated puzzle.
